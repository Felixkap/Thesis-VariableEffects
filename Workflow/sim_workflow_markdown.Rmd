---
title: "Simulation Workflow for an explorative Random Forest Model"
output: pdf_document
author: Felix Kapulla
header-includes:
  - \usepackage{amsmath,amssymb}
  - \usepackage{algorithm}
  - \usepackage[noend]{algpseudocode}
---

```{r, include=F}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

### Import Packages and Helper File
```{r}
library(ranger)
source('C:/Users/kpl/Thesis Project/Thesis-VariableEffects/Workflow/Helper_Workflow.R')
```


## Definition of Main and Low-order Interaction Effects
For any machine learning model $\hat{f}(\cdot)$, a simple main effect for predictor variable $x_j$ can be defined by fixing some point in the middle of the predictor space and shifting the coordinate of interest by some amount to form two new points Q and R. 


\begin{itemize}
    \item $Q_j = ( \hat{\mu}_1, \hat{\mu}_2,..., \hat{\mu_j} - k \cdot \hat{\sigma}_{x_j},...,\hat{\mu}_P )$\\
    \item $R_j = ( \hat{\mu}_1, \hat{\mu}_2,..., \hat{\mu_j} + k \cdot \hat{\sigma}_{x_j},...,\hat{\mu}_P )$ 
\end{itemize}
   
Here, the ML model $\hat{f}(\cdot)$ approximates the true functional relationship between $P$ predictors 
and an outcome variable and $\hat{\mu}_j$, $\hat{\sigma}_j$ are estimates of the true mean and standard deviation of the $j$-th predictor variable respectively. Hence, $Q_j$ and $R_j$ are estimated from data. An effect of $x_j$ on the conditional mean $E(Y \mid X)$ can be estimated as:

\begin{align}
    &\hat{\beta_j} = \frac{\hat{f}(R_j)\ -\ \hat{f}(Q_j)}{2 \cdot k \cdot \hat{\sigma}_{x_j}}
    \label{main}
\end{align}


Similarly to the main effect definition, a low-order interaction effect between two predictor variables can be defined by additionally shifting the coordinate of the interaction variable of interest by some amount as well such that four different data points are formed in total. Hence, a simple low-order interaction effect between two variables can be defined as :

\begin{equation}
    \hat{\gamma}_{i,j} = \frac{\left(\hat{f}(Q_1) - \hat{f}(Q_0)\right) - \left(\hat{f}(R_1) - \hat{f}(R_0)\right)}{ - 4 \cdot k^2 \cdot \hat{\sigma}_{x_i} \cdot \hat{\sigma}_{x_j}}
\label{simple_interaction}    
\end{equation} 

The derivation of the denominator is shown in the appendix of my thesis.


## Jackknife-After Bootstrap: Estimating Standard Errors of Main and Low-Order Interaction Effects
Standard errors of our defined main and low-order interaction effects can be estimated by means of the jackknife-after-bootstrap estimation method. For detailed explanation, have a look at my thesis. There are two options to use the jackknife-after-bootstrap to arrive at a standard error estimated.<br>


1) Estimate Variance-Covariance Matrix between random forest predictions of interest. For the standard error of a main effect compute $\sqrt{\widehat{Var}\left[\frac{\hat{f}(R)\ -\ \hat{f}(Q)}{2 \cdot k \cdot \hat{\sigma}_{x_j}} \right]} = \sqrt{\frac{1}{(2 \cdot k \cdot \hat{\sigma}_{x_j})^2} \left( \widehat{Var}[\hat{f}(R)]+ \widehat{Var}[\hat{f}(Q)]-2 \cdot \widehat{Cov}(\hat{f}(Q),\hat{f}(R))\right)}$. The estimation of low-order interaction effects based on the variance-covariance matrix is shown in my thesis. <br>


Variance estimation of RF predictions for a data point Q (see paper of Wager et. al 2014):
\begin{align}
    &{\hat{V}}_J^B = \frac{n-1}{n} \sum_{i=1}^{n}{({\hat{\theta}}_{(-i)}^B(Q) - {\hat{\theta}}^B(Q))}^2 , \label{Var_j} \\
    &\text{where }\hat{\theta}_{(-i)}^B(Q)=\frac{\sum_{{b: N_{bi}^*=0}}{t_b^*(Q)}}{|{N_{bi}^* = 0}|}\nonumber \\
    &\hat{V}_{J-U}^B = \hat{V}_J^B - (e-1) \frac{n}{B^2}\sum_{b-1}^B \left( t_b^*(Q) - {\hat{\theta}}^B(Q) \right)^2
\end{align}

* $B$: Number of trees used to fit the random forest
* $|{N_{bi}^* = 0}|$ Number of trees in which observation $i$ hasn't been part of
* ${\hat{\theta}}_{(-i)}^B(Q)$: Random Forest prediction for $Q$ based on those trees where training observation $i$ was not part of
* ${\hat{\theta}}^B(Q)$: Random Forest prediction for $Q$
* $t_b^*(Q)$: Single tree prediction for $Q$ of $b$-th tree<br>

Covariance estimation between RF predictions for data points Q and R (modified version of jackknife-after-bootstrap) 
\begin{align}
    &{\widehat{Cov}}_J^B = \frac{n-1}{n}\sum_{i=1}^{n}{\left({\hat{\theta}}_{\left(-i\right)}^B\left(Q\right)-{\hat{\theta}}^B\left(Q\right)\right)\cdot\left({\hat{\theta}}_{\left(-i\right)}^B\left(R\right)-{\hat{\theta}}^B\left(R\right)\right)} \label{Cov_RF_Predictions} \\
    &{\widehat{Cov}}_{J-U}^B = \widehat{Cov}_{J}^B - (e-1)\frac{n}{B^2} \sum_{b-1}^B \left( t_b^*(Q) - {\hat{\theta}}^B(Q) \right) \left( t_b^*(R) - {\hat{\theta}}^B(R) \right)
\label{cov_rf_predictions}    
\end{align}


2) Use the jackknife-after-bootstrap procedure to estimate variable effects directly (modified version of jackknife-after-bootstrap) 

\begin{align}
    &{\widehat{V}\left(\hat{\beta_j}\right)}_J^B = \frac{n-1}{n}\sum_{i=1}^{n} \left( \hat{\beta_j}_{(-i)} - \hat{\beta_j}\right)^2 \label{Main_Effect_Variance} \\ 
    &{\widehat{V}\left(\hat{\beta_j}\right)}_J^B = \frac{n-1}{n}\sum_{i=1}^{n}{\left(\frac{{\hat{\theta}}_{\left(-i\right)}^B\left(R_j\right) - {\hat{\theta}}_{\left(-i\right)}^B\left(Q_j\right)}{2k\hat{\sigma_x}} - \frac{{\hat{\theta}}^B\left(R_j\right) - {\hat{\theta}}^B\left(Q_j\right)}{2k\hat{\sigma_x}} \right)^2} \label{Main_Effect_Variance_math} \\
    &{\widehat{V}\left(\hat{\beta_j}\right)}_{J-U}^B = {\widehat{V}\left(\hat{\beta_j}\right)}_J^B - (e-1)\frac{n}{B^2} \sum_{b-1}^B \left( \frac{t_b^*(R_j) - t_b^*(Q_j)}{2k\hat{\sigma_x}} - \frac{t^*(R_j) - t^*(Q_j)}{2k\hat{\sigma_x}} \right)^2
\label{Main_Effect_Variance_Bias_Corrected}    
\end{align}

Both estimation methods yield the same results! This is also shown a bit in later in this workflow.


## Generate Data
Preditors are sampled from a $X \sim \mathcal{N}(\mathbf{0},\Sigma)$ and errors from $X_j \sim \mathcal{N}(0,\sigma_{\epsilon})$. The number of training and test observations are determined by parameters *N_train* and *N_test* respectively. Furthermore, *cat* determines how many binary categorical are sampled. Levels of a categorical variables are sampled with equal probability and noted as 0 and 1. Parameter *k* determines over which range the variable effect for numerical variables are computed. Lastly, the functional relationship can be defined manually such that $\mathbf{y}=f(\mathbf{X})+\mathbf{e}$.
```{r}
## @param seed: Seed Number to obtain same data set
## @param N_train: Number of training observations
## @param N_test: Number of test observations
## @param cor: Correlation between features
## @param sigma_e: Noise in the data (standard deviation of errors)
## @param formula: Functional Relationship between predictors and outcome
## @param cat: Number of binary categorical variables to include as covariates
## @param k: Range over which the variable effect is to be computed

seed <- 12391
set.seed(seed)
N_train <- 500
N_test <- 200
cor <- 0.8
sigma_e <- 3
formula <- c("4*x.1+0.2*x.2-5*x.3+2*x.4-2*x.3*x.4")
cat <- 1
k <- 1 

# Obtain number of variables: n_vars
n_vars <- length(unique(unlist(str_extract_all(formula,"x.[0-9]+")))) 


train_data <- generate_data(N_train, n_vars, cor, formula, sigma_e, cat=cat)
test_data <- generate_data(N_test, n_vars, cor, formula, sigma_e, cat=cat)
print(head(train_data$data))
```

## Set Hyperparameter of Random Forest
The random forest algorithm has several hyperparameters of which the most important ones are the number of trees used to fit the random forest (*num.trees*), the number of randomly sampled variables that are considered when looking for the best split (*mtry*), the minimum number of observations required to split an internal node (*node.size*) as well as the maximal tree depth (*max.depth*). An alternative to just set hyperparameters manually is to tune them. In the world of machine learning the full training set is usually split into a train and test set and on the reduced training set cross-validation is commonly performed to tune hyperparameters. 

```{r}
## @param num.trees: Number of trees used to fit the Random Forest
## @param mtry: Number of variables to possibly split at in each node 
#               ---> Smaller value -> more randomness <=> Uncorrelated trees ->
## @param node.size: Minimal node size to split at. Default 1 for classification, 5 for regression
## @param max.depth: Maximal tree depth


num.trees <- 2500
mtry <- 2
node.size <- 5
max.depth <- NULL # Corresponds to unlimited depth
```


## Fit Random Forest
After specifying the hyperparameters the random forest is fit on the training data. F0r this, the ranger function is used from the ranger package. Ranger is a fast implementation of random forests (Breiman 2001). It is faster than the most common *randomForest* function from the package *randomForest*. Classification, regression, and survival forests are supported. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001). Furthermore, implementations of extremely randomized trees (Geurts et al. 2006) and quantile regression forests (Meinshausen 2006) are included.
```{r}
# Random Forest
rf <- ranger( formula = y ~ .,
              data = train_data$data,
              num.trees = num.trees,
              keep.inbag = T, 
              oob.error = T, 
              importance = 'permutation',
              min.node.size = node.size,
              max.depth = max.depth)
```


## Performance on Training and Test Data
Based on the random forest predictions the root mean squared error (RMSE) or $R^2$ can be computed as metric for the model fit. Furthermore, it is useful to plot the rmse against the number of trees. This running rmse estimate shows at how many trees the training or test error starts to stabilize. More important than the model fit on the train data is the performance on test data in order to evaluate how well the machine learning model is able to generalize to unseen data. 
```{r}
# Random Forest on Train Data
all_tree_predictions <- predict(rf, train_data$x, predict.all = T)$predictions
predictions.rf <- rowMeans(all_tree_predictions)
rmse_0 <- sqrt(mean((train_data$y - predictions.rf)^2))
               
cat("RMSE on train data of Random Forest:", rmse_0,
    ". R^2 of Random Forest:", 1 - mean((train_data$y - predictions.rf)^2) / var(train_data$y),
    ".\nOOB estimate of RF for generalization error:", sqrt(rf$prediction.error), ".")

# Plot RMSE vs Number of Trees (train data)
plot_error_rate(rf, train_data$data, data_type='train', thinning=10)




# Random Forest on Test Data
all_tree_predictions <- predict(rf, test_data$x, predict.all = T)$predictions
predictions.rf <- rowMeans(all_tree_predictions)

cat("RMSE of Random Forest on independent test data:", sqrt(mean((test_data$y - predictions.rf)^2)))

# Plot RMSE vs Number of Trees (train data)
plot_error_rate(rf, test_data$data, data_type='test', thinning=10)

```


## Summary of Model fit 
Obtain summary of effect estimates with corresponding standard errors (estimated by jackknife-after-bootstrap). This is done via the user-defined function *summary.rf*. The user has several options to specify the summary output. Specification Options are the following:<br>

* *point_of_interest*: Serves as reference point to compute variable main and low-order interaction effects. Default is the mean of all numerical and mode of all binary categorical variables. 
* *interaction*: Specifiy if interaction effects should be estimated. This argument can be specified as 'None', 'specific' or 'all'.
* *interaction_terms*: If Argument *interaction* was specified as 'specific', then the user should specify which interaction effects should be estimated by means of a character vector that is of the form 'variable.name1:variable.name2' 
* *moving_var_effects*: Boolean which indicates whether a running variance estimate of main and low-order interaction effects should be estimated (VAR-Estimate vs. Number of Trees)
* *moving_se_effects*: Boolean which indicates whether a running standard error estimate of main and low-order interaction effects should be estimated (SE-Estimate vs. Number of Trees)
* *first_tree*: Define which should be the starting tree for plots (in the beginning there might be lots of variation)
* *thinning*: Parameter that defines that only every x trees moving se's / var's should be computed

Output of function *summary.rf* is a list with the following components:<br>

* *$\$$summary*: Table of estimated main and low-order interaction effects with corresponding standard errors (estimated by direct version of jackknife-after-bootstrap)
* *$\$$cov_list*: listed are data points that were used to compute each effect. Next to it, the estimated variance covariance matrix of the corresponding random forest predictions is provided.
* *$\$$moving_var_effects* or *$\$$moving_se_effects*: Plots that illustrate running Var- / Se- estimate of variable effects to check whether estimates are stable.
```{r}
system.time(result <- summary.rf(x = train_data$x, 
               fitted.rf = rf, 
               point_of_interest = 'mean_mode',
               k = k, 
               interaction = 'specific', 
               interaction_terms = 'x.1:x.2', 
               moving_var_effects = T, 
               moving_se_effects = F, 
               first_tree=100, 
               thinning=50))

# Summary output of variable main and low-order interaction effects
result$summary
# List containing the data points with corresponding variance-covariance matrix 
# that were used to compute variable main and low order interaction effects 
result$cov_list
# Plot moving variance or standard error estimates of effects
result$moving_var_effect
```

Note that standard errors in the summary table are estimated by means of the direct jackknife-after-bootstrap application according to equation \ref{Main_Effect_Variance_Bias_Corrected}. We could also use the output component *$\$$cov_list* to compute the standard error estimate by hand. *$\$$cov_list* contains data points that were used to compute each variable effect. Next to it, the variance covariance matrices of the corresponding random forest predictions are provided. These variance covariance matrices were estimated according to equations \ref{Var_j} and \ref{cov_rf_predictions}.<br> 

The code snippet below illustrates how to estimate a standard error of a variable main effect for variable $x_1$ (this is what happens inside the function *summary.rf*) by the direct version of the jackknife-after-bootstrap method. This is done via the user-defined function *RangerForestPredict*. Note, that the variable effect itself must be computed separately based on the data points stored in *result$cov_list$x.1$effect_points*. To estimate the standard error of the effect for $x_j$ you must provide the following arguments to *RangerForestPredict*:<br>

* The fitted random forest object from the *ranger* package
* The data points that were used to compute a main or low-order interaction effect (dataframe of 2 or 4 rows)
* argument *type* must be set to 'se'
* argument *se.method* must be set to 'se_direct
* Inbag counts of the fitted random forest object 

```{r}
RangerForestPredict(object = rf$forest, 
                    data = result$cov_list$x.1$effect_points, 
                    predict.all = T, 
                    type = 'se', 
                    se.method = 'se_direct', 
                    inbag.counts = rf$inbag.counts)$se_effect
```


We could also estimate the standard error based on the jackknifed variance and covariance estimates
```{r}
RangerForestPredict(object = rf$forest, 
                    data = result$cov_list$x.1$effect_points, 
                    predict.all = T, 
                    type = 'se', 
                    se.method = 'jack_cov', 
                    inbag.counts = rf$inbag.counts)$cov

```
Note that this is the same matrix as you can find in *result$\$$cov$\_$list$\$$x.1$\$$cov*. Based on this variance-covariance matrix we could recover the standard error estimate for variable x.1 as follows
```{r}
sqrt(1 / (2*k*sd(train_data$x$x.1))^2 *
       (result$cov_list$x.1$cov[1,1]+result$cov_list$x.1$cov[2,2]
        -2*result$cov_list$x.1$cov[2,1]))
```


Last note! The R-build in function to estimate the variance of random forest predictions is as follows:
```{r}
(predict(object = rf, 
        data = result$cov_list$x.1$effect_points, 
        predict.all = T, 
        type = 'se', 
        se.method = 'jack', 
        inbag.counts = rf$inbag.counts)$se)^2
```
Great, theses variance estimates are the same as the diagonal values in the prior variance-covariance matrix!

## Partial Dependence Plots 

In order to visualize possibly complex, non-linear relationships between predictors and the predicted outcome variable, partial dependence plots (PDP) can be used. A PDP is a type of global model-agnostic method which shows the marginal effect of usually one or two features on the predicted outcome of any machine learning model. Given a regression task, the partial dependence function can be estimated by looping over all training observations (average over marginal distribution of all other variables that are not plotted):


\begin{algorithm}
\caption{Estimation of Partial Dependence Function}
\label{alg:PDP}
\begin{algorithmic}[1]
\State {\bfseries Input:} Feature matrix $\boldsymbol{X}$, Trained prediction model $\hat{f}(\boldsymbol{X})$, Range of grid values for $X_S$: $x_s^G$, $N$ Training Observations
\For{$x_S \in x_S^G$}
\For{$i$ in $1:N$}
    \State $x_S^{(i)} \leftarrow x_S$
    \State $\hat{y}_i^{PDP} = \hat{f}\left( x_S^{(i)}, x_C^{(i)}\right)$
    \EndFor
    \State $\hat{f}\left( x_S\right) = \frac{1}{n} \sum_{i=1}^n \hat{y}_i^{PDP}$
    \label{PDP-func}
    \EndFor  
\end{algorithmic}
\end{algorithm}

```{r}
# PDP of each single variables
plot_pdp(rf, train_data$x, names(train_data$x))

# PDP of two specific variable
#plot_2pdp(rf, c('x1a', 'x4'))
```


## Permutation Variable Importance

The basic idea permutation variable importance is to consider a feature $x_j$ important if it has a positive effect on the prediction performance. For that, values of $x_j$ are permuted to break the association between that feature and the outcome. Hence, if the model relied for its predictions on $x_j$ , then permuting its values would result in an increase of the model’s prediction error.


\begin{algorithm} 
\caption{Permutation Variable Importance Algorithm}
\label{alg:PVIA}
\begin{algorithmic}[1]
\State {\bfseries Input:} Feature matrix $\boldsymbol{X}$, trained model $\hat{f}(\boldsymbol{X})$, outcome vector $y$, loss function $\boldsymbol{L}\left(y, \hat{f}(\boldsymbol{X}) \right)$
\State Estimate original loss for $\hat{f}(\boldsymbol{X})$: $\boldsymbol{L}_0\left(y, \hat{f}(\boldsymbol{X}) \right)$
\For{Feature $j \in {1,...,P}$}
    \State Generate random permutation vector: $\overrightarrow{r}$
    \State Generate feature matrix $\boldsymbol{X}_{\overrightarrow{r}}$ by permuting $j$-th feature according to  $\overrightarrow{r}$
    \State Estimate new prediction error for $\hat{f}\left( \boldsymbol{X}_{\overrightarrow{r}}\right)$: $ \boldsymbol{L}\left(y, \hat{f}\left( \boldsymbol{X}_{\overrightarrow{r}}\right) \right)$ 
    \State Compute Permutation Variable Importance as $PVI_j = \boldsymbol{L}\left(y, \hat{f}\left( \boldsymbol{X}_{\overrightarrow{r}}\right) -\right)\boldsymbol{L}_0\left(y, \hat{f}(\boldsymbol{X}) \right)$ 
    \label{alg:line7}
    \EndFor
\end{algorithmic}
\end{algorithm}



```{r}
# Visualizing Variable Importance
plot_varimp(rf)
```




