---
title: "Untitled"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, warning=FALSE,fig.width=14, fig.height=14}
library(Matrix)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(ranger)
library(MixMatrix)
library(mvtnorm)
library(stringr)
library(ggh4x)
library(iml)
library(ALEPlot)




#true_effects <- c(3,2,3,4)
effect_sizes <- c("-5", "-3","-1", "+1", "+3", "+5")
final_plot_list <- vector(mode = 'list', length = length(effect_sizes))
cnt <- 1
for (e in effect_sizes) {
  true_effects <- c(1, 1, as.integer(e))
  N <- c(10) ; num.trees <- 200; cor <- c(0.9)
  k <- c(1); node_size <- c(5)
  grid_size <- seq(-3,3,length.out=20)
  formula <- paste0("x.1+x.2", e, "*x.3")
  n_vars <- length(unique(unlist(str_extract_all(formula,"x.[0-9]+")))) 
  reps <- 20
  
  dat_list <- vector(mode = 'list', length = reps)
  dat_list <- lapply(X = 1:reps, FUN = function(X){
    dat_list[[X]] = vector(mode = 'list', length =n_vars)
  })
  for (rep in 1:reps) {
    
    if (n_vars == 1) {
      # Normal Distribution with Mean 0 and Variance 1
      x <- data.frame(x.1 = rnorm(N, 0, 1))
    } else{
      # Multivariate Normal Distribution with Mean Vector 0 and diagonal Variance Covariance Matrix 
      x <- data.frame(x = rmvnorm(N, mean = rep(0, n_vars), sigma = CSgenerate(n_vars, cor)))
    }
    
    # Get interaction terms
    all_terms <- unlist(strsplit(formula, "(?<=.)(?=[+])|(?<=.)(?=[-])",perl = TRUE))
    n_coefs <- length(all_terms)
    for (term in all_terms) {
      if (length(unique(unlist(str_extract_all(term,"x.[0-9]+")))) > 1) {
        interaction_term <- unique(unlist(str_extract_all(term,"x.[0-9]+")))
      }
    }
    
    formula_p <- parse(text = formula)
    y <- eval(formula_p, x) + rnorm(N, 0, 1)
    data <- data.frame(cbind(x, y))
    
    
    # rf <- ranger( formula = y ~ .,
    #               data = data,
    #               num.trees = num.trees,
    #               keep.inbag = T, 
    #               oob.error = F, # Save computational time
    #               min.node.size = node_size)
    
    lrn = mlr::makeLearner("regr.randomForest")
    task <- mlr::makeRegrTask(data=data, target="y")
    rf <- mlr::train(learner = lrn, task = task)
    
    
    
    
    pred <- Predictor$new(rf, data=x, y=y)
    
    for (j in 1:n_vars) {
      mod <- FeatureEffect$new(pred, feature = paste0('x.',j), method = "pdp", grid.points = grid_size)
      dat_list[[rep]][[j]] <- mod$results[,1:2]
    }
    
  }
  print('DONE')
  
  var_list <- vector(mode = 'list', length = n_vars)
  var_list <- lapply(X = 1:n_vars, FUN = function(X){
    var_list[[X]] = vector(mode = 'list', length = reps)
  })
  
  for (j in 1:n_vars) {
    for (rep in 1:reps) {
      dat <- dat_list[[rep]][[j]]
      names(dat) <- c('x', 'values')
      var_list[[j]][[rep]] <- cbind(dat, rep = rep, variable = paste0('x', j))
    }
  }
  
  
  
  plot_list <- vector(mode = 'list', length = n_vars)
  for (j in 1:n_vars) {
    
    true_effect <- true_effects[j]
    dat <- do.call('rbind', var_list[[j]])
    plot_list[[j]] <- ggplot(dat) +
      geom_line(aes_string(x='x', y='values',group='rep'), alpha = 0.3) +
      geom_smooth(formula = y ~ x, aes_string(x='x', y='values'), method="loess", se=F) +
      geom_abline(slope = true_effect, colour = 'red') + 
      labs(x = paste0('X', j), 
           y = 'PDP Values') + 
      theme_bw()
  }
  
  final_plot_list[[cnt]] <- plot_list
  cnt <- cnt + 1
  
  
}

mygg <- function(x){
  ggarrange(plotlist =  x, nrow = 1)
}

ggarrange(plotlist = lapply(X = final_plot_list, FUN = mygg), nrow = length(final_plot_list))
```





